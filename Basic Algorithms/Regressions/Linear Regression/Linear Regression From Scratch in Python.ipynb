{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will implement stochastic gradient descent to optimize a linear regression algorithm from scratch with Python.\n",
    "\n",
    "###### Linear regression is a technique where a straight line is used to model the relationship between input and output values. In more than two dimensions, this straight line may be thought of as a plane or hyperplane. Predictions are made as a combination of the input values to predict the output value. Each input attribute (x) is weighted using a coefficient (b), and the goal of the learning algorithm is to discover a set of coefficients that results in good predictions (y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a prediction with coefficients\n",
    "def predict(row, coefficients):\n",
    "\tyhat = coefficients[0]\n",
    "\tfor i in range(len(row)-1):\n",
    "\t\tyhat += coefficients[i + 1] * float(row[i])\n",
    "\treturn yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected=1.000, Predicted=1.200\n",
      "Expected=3.000, Predicted=2.000\n",
      "Expected=3.000, Predicted=3.600\n",
      "Expected=2.000, Predicted=2.800\n",
      "Expected=5.000, Predicted=4.400\n"
     ]
    }
   ],
   "source": [
    "dataset = [[1, 1], [2, 3], [4, 3], [3, 2], [5, 5]]\n",
    "coef = [0.4, 0.8]\n",
    "for row in dataset:\n",
    "\tyhat = predict(row, coef)\n",
    "\tprint(\"Expected=%.3f, Predicted=%.3f\" % (row[-1], yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimating Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate linear regression coefficients using stochastic gradient descent\n",
    "def coefficients_sgd(train, l_rate, n_epoch):\n",
    "\tcoef = [0.0 for i in range(len(train[0]))]\n",
    "\tfor epoch in range(n_epoch):\n",
    "\t\tsum_error = 0\n",
    "\t\tfor row in train:\n",
    "\t\t\tyhat = predict(row, coef)\n",
    "\t\t\terror = yhat - row[-1]\n",
    "\t\t\tsum_error += error**2\n",
    "\t\t\tcoef[0] = coef[0] - l_rate * error\n",
    "\t\t\tfor i in range(len(row)-1):\n",
    "\t\t\t\tcoef[i + 1] = coef[i + 1] - l_rate * error * float(row[i])\n",
    "\t\tprint('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error))\n",
    "\treturn coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">epoch=0, lrate=0.001, error=46.236\n",
      ">epoch=1, lrate=0.001, error=41.305\n",
      ">epoch=2, lrate=0.001, error=36.930\n",
      ">epoch=3, lrate=0.001, error=33.047\n",
      ">epoch=4, lrate=0.001, error=29.601\n",
      ">epoch=5, lrate=0.001, error=26.543\n",
      ">epoch=6, lrate=0.001, error=23.830\n",
      ">epoch=7, lrate=0.001, error=21.422\n",
      ">epoch=8, lrate=0.001, error=19.285\n",
      ">epoch=9, lrate=0.001, error=17.389\n",
      ">epoch=10, lrate=0.001, error=15.706\n",
      ">epoch=11, lrate=0.001, error=14.213\n",
      ">epoch=12, lrate=0.001, error=12.888\n",
      ">epoch=13, lrate=0.001, error=11.712\n",
      ">epoch=14, lrate=0.001, error=10.668\n",
      ">epoch=15, lrate=0.001, error=9.742\n",
      ">epoch=16, lrate=0.001, error=8.921\n",
      ">epoch=17, lrate=0.001, error=8.191\n",
      ">epoch=18, lrate=0.001, error=7.544\n",
      ">epoch=19, lrate=0.001, error=6.970\n",
      ">epoch=20, lrate=0.001, error=6.461\n",
      ">epoch=21, lrate=0.001, error=6.009\n",
      ">epoch=22, lrate=0.001, error=5.607\n",
      ">epoch=23, lrate=0.001, error=5.251\n",
      ">epoch=24, lrate=0.001, error=4.935\n",
      ">epoch=25, lrate=0.001, error=4.655\n",
      ">epoch=26, lrate=0.001, error=4.406\n",
      ">epoch=27, lrate=0.001, error=4.186\n",
      ">epoch=28, lrate=0.001, error=3.990\n",
      ">epoch=29, lrate=0.001, error=3.816\n",
      ">epoch=30, lrate=0.001, error=3.662\n",
      ">epoch=31, lrate=0.001, error=3.525\n",
      ">epoch=32, lrate=0.001, error=3.404\n",
      ">epoch=33, lrate=0.001, error=3.296\n",
      ">epoch=34, lrate=0.001, error=3.200\n",
      ">epoch=35, lrate=0.001, error=3.115\n",
      ">epoch=36, lrate=0.001, error=3.040\n",
      ">epoch=37, lrate=0.001, error=2.973\n",
      ">epoch=38, lrate=0.001, error=2.914\n",
      ">epoch=39, lrate=0.001, error=2.862\n",
      ">epoch=40, lrate=0.001, error=2.815\n",
      ">epoch=41, lrate=0.001, error=2.773\n",
      ">epoch=42, lrate=0.001, error=2.737\n",
      ">epoch=43, lrate=0.001, error=2.704\n",
      ">epoch=44, lrate=0.001, error=2.675\n",
      ">epoch=45, lrate=0.001, error=2.650\n",
      ">epoch=46, lrate=0.001, error=2.627\n",
      ">epoch=47, lrate=0.001, error=2.607\n",
      ">epoch=48, lrate=0.001, error=2.589\n",
      ">epoch=49, lrate=0.001, error=2.573\n",
      "[0.22998234937311363, 0.8017220304137576]\n"
     ]
    }
   ],
   "source": [
    "# Calculate coefficients\n",
    "dataset = [[1, 1], [2, 3], [4, 3], [3, 2], [5, 5]]\n",
    "l_rate = 0.001\n",
    "n_epoch = 50\n",
    "coef = coefficients_sgd(dataset, l_rate, n_epoch)\n",
    "print(coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression With Stochastic Gradient Descent for Wine Quality\n",
    "from random import seed\n",
    "from random import randrange\n",
    "from csv import reader\n",
    "from math import sqrt\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a CSV file\n",
    "def load_csv(filename):\n",
    "\tdataset = list()\n",
    "\twith open(filename, 'r') as file:\n",
    "\t\tcsv_reader = reader(file)\n",
    "\t\tfor row in csv_reader:\n",
    "\t\t\tif not row:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tdataset.append(row)\n",
    "\treturn dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert string column to float\n",
    "def str_column_to_float(dataset, column):\n",
    "\tfor row in dataset:\n",
    "\t\trow[column] = float(row[column].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the min and max values for each column\n",
    "def dataset_minmax(dataset):\n",
    "\tminmax = list()\n",
    "\tfor i in range(len(dataset[0])):\n",
    "\t\tcol_values = [float(row[i]) for row in dataset]\n",
    "\t\tvalue_min = min(col_values)\n",
    "\t\tvalue_max = max(col_values)\n",
    "\t\tminmax.append([value_min, value_max])\n",
    "\treturn minmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rescale dataset columns to the range 0-1\n",
    "def normalize_dataset(dataset, minmax):\n",
    "\tfor row in dataset:\n",
    "\t\tfor i in range(1,len(row)):\n",
    "\t\t\trow[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split a dataset into k folds\n",
    "def cross_validation_split(dataset, n_folds):\n",
    "\tdataset_split = list()\n",
    "\tdataset_copy = list(dataset)\n",
    "\tfold_size = int(len(dataset) / n_folds)\n",
    "\tfor i in range(n_folds):\n",
    "\t\tfold = list()\n",
    "\t\twhile len(fold) < fold_size:\n",
    "\t\t\tindex = randrange(len(dataset_copy))\n",
    "\t\t\tfold.append(dataset_copy.pop(index))\n",
    "\t\tdataset_split.append(fold)\n",
    "\treturn dataset_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate root mean squared error\n",
    "def rmse_metric(actual, predicted):\n",
    "\tsum_error = 0.0\n",
    "\tfor i in range(len(actual)):\n",
    "\t\tprediction_error = predicted[i] - actual[i]\n",
    "\t\tsum_error += (prediction_error ** 2)\n",
    "\tmean_error = sum_error / float(len(actual))\n",
    "\treturn sqrt(mean_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate an algorithm using a cross validation split\n",
    "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
    "\tfolds = cross_validation_split(dataset, n_folds)\n",
    "\tscores = list()\n",
    "\tfor fold in folds:\n",
    "\t\ttrain_set = list(folds)\n",
    "\t\ttrain_set.remove(fold)\n",
    "\t\ttrain_set = sum(train_set, [])\n",
    "\t\ttest_set = list()\n",
    "\t\tfor row in fold:\n",
    "\t\t\trow_copy = list(row)\n",
    "\t\t\ttest_set.append(row_copy)\n",
    "\t\t\trow_copy[-1] = None\n",
    "\t\tpredicted = algorithm(train_set, test_set, *args)\n",
    "\t\tactual = [row[-1] for row in fold]\n",
    "\t\trmse = rmse_metric(actual, predicted)\n",
    "\t\tscores.append(rmse)\n",
    "\treturn scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression Algorithm With Stochastic Gradient Descent\n",
    "def linear_regression_sgd(train, test, l_rate, n_epoch):\n",
    "\tpredictions = list()\n",
    "\tcoef = coefficients_sgd(train, l_rate, n_epoch)\n",
    "\tfor row in test:\n",
    "\t\tyhat = predict(row, coef)\n",
    "\t\tpredictions.append(yhat)\n",
    "\treturn(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression on wine quality dataset\n",
    "seed(1)\n",
    "# load and prepare data\n",
    "filename = 'winequality-white.csv'\n",
    "dataset = load_csv(filename)\n",
    "for i in range(len(dataset[0])):\n",
    "    if(i==0):\n",
    "        continue\n",
    "    str_column_to_float(dataset, i)\n",
    "# normalize\n",
    "minmax = dataset_minmax(dataset)\n",
    "normalize_dataset(dataset, minmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">epoch=0, lrate=0.010, error=106.457\n",
      ">epoch=1, lrate=0.010, error=91.630\n",
      ">epoch=2, lrate=0.010, error=88.803\n",
      ">epoch=3, lrate=0.010, error=87.274\n",
      ">epoch=4, lrate=0.010, error=86.378\n",
      ">epoch=5, lrate=0.010, error=85.834\n",
      ">epoch=6, lrate=0.010, error=85.492\n",
      ">epoch=7, lrate=0.010, error=85.268\n",
      ">epoch=8, lrate=0.010, error=85.117\n",
      ">epoch=9, lrate=0.010, error=85.012\n",
      ">epoch=10, lrate=0.010, error=84.935\n",
      ">epoch=11, lrate=0.010, error=84.879\n",
      ">epoch=12, lrate=0.010, error=84.835\n",
      ">epoch=13, lrate=0.010, error=84.800\n",
      ">epoch=14, lrate=0.010, error=84.772\n",
      ">epoch=15, lrate=0.010, error=84.749\n",
      ">epoch=16, lrate=0.010, error=84.729\n",
      ">epoch=17, lrate=0.010, error=84.712\n",
      ">epoch=18, lrate=0.010, error=84.696\n",
      ">epoch=19, lrate=0.010, error=84.682\n",
      ">epoch=20, lrate=0.010, error=84.670\n",
      ">epoch=21, lrate=0.010, error=84.658\n",
      ">epoch=22, lrate=0.010, error=84.647\n",
      ">epoch=23, lrate=0.010, error=84.637\n",
      ">epoch=24, lrate=0.010, error=84.627\n",
      ">epoch=25, lrate=0.010, error=84.618\n",
      ">epoch=26, lrate=0.010, error=84.609\n",
      ">epoch=27, lrate=0.010, error=84.600\n",
      ">epoch=28, lrate=0.010, error=84.592\n",
      ">epoch=29, lrate=0.010, error=84.584\n",
      ">epoch=30, lrate=0.010, error=84.576\n",
      ">epoch=31, lrate=0.010, error=84.568\n",
      ">epoch=32, lrate=0.010, error=84.560\n",
      ">epoch=33, lrate=0.010, error=84.552\n",
      ">epoch=34, lrate=0.010, error=84.545\n",
      ">epoch=35, lrate=0.010, error=84.537\n",
      ">epoch=36, lrate=0.010, error=84.530\n",
      ">epoch=37, lrate=0.010, error=84.523\n",
      ">epoch=38, lrate=0.010, error=84.515\n",
      ">epoch=39, lrate=0.010, error=84.508\n",
      ">epoch=40, lrate=0.010, error=84.501\n",
      ">epoch=41, lrate=0.010, error=84.494\n",
      ">epoch=42, lrate=0.010, error=84.487\n",
      ">epoch=43, lrate=0.010, error=84.480\n",
      ">epoch=44, lrate=0.010, error=84.473\n",
      ">epoch=45, lrate=0.010, error=84.467\n",
      ">epoch=46, lrate=0.010, error=84.460\n",
      ">epoch=47, lrate=0.010, error=84.453\n",
      ">epoch=48, lrate=0.010, error=84.447\n",
      ">epoch=49, lrate=0.010, error=84.440\n",
      ">epoch=0, lrate=0.010, error=103.523\n",
      ">epoch=1, lrate=0.010, error=88.510\n",
      ">epoch=2, lrate=0.010, error=85.732\n",
      ">epoch=3, lrate=0.010, error=84.269\n",
      ">epoch=4, lrate=0.010, error=83.432\n",
      ">epoch=5, lrate=0.010, error=82.936\n",
      ">epoch=6, lrate=0.010, error=82.632\n",
      ">epoch=7, lrate=0.010, error=82.439\n",
      ">epoch=8, lrate=0.010, error=82.311\n",
      ">epoch=9, lrate=0.010, error=82.223\n",
      ">epoch=10, lrate=0.010, error=82.160\n",
      ">epoch=11, lrate=0.010, error=82.114\n",
      ">epoch=12, lrate=0.010, error=82.078\n",
      ">epoch=13, lrate=0.010, error=82.049\n",
      ">epoch=14, lrate=0.010, error=82.025\n",
      ">epoch=15, lrate=0.010, error=82.005\n",
      ">epoch=16, lrate=0.010, error=81.987\n",
      ">epoch=17, lrate=0.010, error=81.971\n",
      ">epoch=18, lrate=0.010, error=81.957\n",
      ">epoch=19, lrate=0.010, error=81.944\n",
      ">epoch=20, lrate=0.010, error=81.932\n",
      ">epoch=21, lrate=0.010, error=81.920\n",
      ">epoch=22, lrate=0.010, error=81.909\n",
      ">epoch=23, lrate=0.010, error=81.898\n",
      ">epoch=24, lrate=0.010, error=81.888\n",
      ">epoch=25, lrate=0.010, error=81.878\n",
      ">epoch=26, lrate=0.010, error=81.869\n",
      ">epoch=27, lrate=0.010, error=81.859\n",
      ">epoch=28, lrate=0.010, error=81.850\n",
      ">epoch=29, lrate=0.010, error=81.841\n",
      ">epoch=30, lrate=0.010, error=81.832\n",
      ">epoch=31, lrate=0.010, error=81.823\n",
      ">epoch=32, lrate=0.010, error=81.814\n",
      ">epoch=33, lrate=0.010, error=81.806\n",
      ">epoch=34, lrate=0.010, error=81.797\n",
      ">epoch=35, lrate=0.010, error=81.788\n",
      ">epoch=36, lrate=0.010, error=81.780\n",
      ">epoch=37, lrate=0.010, error=81.772\n",
      ">epoch=38, lrate=0.010, error=81.763\n",
      ">epoch=39, lrate=0.010, error=81.755\n",
      ">epoch=40, lrate=0.010, error=81.747\n",
      ">epoch=41, lrate=0.010, error=81.739\n",
      ">epoch=42, lrate=0.010, error=81.731\n",
      ">epoch=43, lrate=0.010, error=81.722\n",
      ">epoch=44, lrate=0.010, error=81.714\n",
      ">epoch=45, lrate=0.010, error=81.706\n",
      ">epoch=46, lrate=0.010, error=81.699\n",
      ">epoch=47, lrate=0.010, error=81.691\n",
      ">epoch=48, lrate=0.010, error=81.683\n",
      ">epoch=49, lrate=0.010, error=81.675\n",
      ">epoch=0, lrate=0.010, error=105.047\n",
      ">epoch=1, lrate=0.010, error=89.620\n",
      ">epoch=2, lrate=0.010, error=86.763\n",
      ">epoch=3, lrate=0.010, error=85.267\n",
      ">epoch=4, lrate=0.010, error=84.411\n",
      ">epoch=5, lrate=0.010, error=83.903\n",
      ">epoch=6, lrate=0.010, error=83.592\n",
      ">epoch=7, lrate=0.010, error=83.395\n",
      ">epoch=8, lrate=0.010, error=83.266\n",
      ">epoch=9, lrate=0.010, error=83.179\n",
      ">epoch=10, lrate=0.010, error=83.119\n",
      ">epoch=11, lrate=0.010, error=83.076\n",
      ">epoch=12, lrate=0.010, error=83.044\n",
      ">epoch=13, lrate=0.010, error=83.020\n",
      ">epoch=14, lrate=0.010, error=83.001\n",
      ">epoch=15, lrate=0.010, error=82.985\n",
      ">epoch=16, lrate=0.010, error=82.972\n",
      ">epoch=17, lrate=0.010, error=82.961\n",
      ">epoch=18, lrate=0.010, error=82.951\n",
      ">epoch=19, lrate=0.010, error=82.942\n",
      ">epoch=20, lrate=0.010, error=82.934\n",
      ">epoch=21, lrate=0.010, error=82.927\n",
      ">epoch=22, lrate=0.010, error=82.920\n",
      ">epoch=23, lrate=0.010, error=82.913\n",
      ">epoch=24, lrate=0.010, error=82.906\n",
      ">epoch=25, lrate=0.010, error=82.900\n",
      ">epoch=26, lrate=0.010, error=82.894\n",
      ">epoch=27, lrate=0.010, error=82.888\n",
      ">epoch=28, lrate=0.010, error=82.882\n",
      ">epoch=29, lrate=0.010, error=82.876\n",
      ">epoch=30, lrate=0.010, error=82.871\n",
      ">epoch=31, lrate=0.010, error=82.865\n",
      ">epoch=32, lrate=0.010, error=82.860\n",
      ">epoch=33, lrate=0.010, error=82.854\n",
      ">epoch=34, lrate=0.010, error=82.849\n",
      ">epoch=35, lrate=0.010, error=82.843\n",
      ">epoch=36, lrate=0.010, error=82.838\n",
      ">epoch=37, lrate=0.010, error=82.833\n",
      ">epoch=38, lrate=0.010, error=82.828\n",
      ">epoch=39, lrate=0.010, error=82.822\n",
      ">epoch=40, lrate=0.010, error=82.817\n",
      ">epoch=41, lrate=0.010, error=82.812\n",
      ">epoch=42, lrate=0.010, error=82.807\n",
      ">epoch=43, lrate=0.010, error=82.802\n",
      ">epoch=44, lrate=0.010, error=82.797\n",
      ">epoch=45, lrate=0.010, error=82.792\n",
      ">epoch=46, lrate=0.010, error=82.787\n",
      ">epoch=47, lrate=0.010, error=82.782\n",
      ">epoch=48, lrate=0.010, error=82.777\n",
      ">epoch=49, lrate=0.010, error=82.772\n",
      ">epoch=0, lrate=0.010, error=104.261\n",
      ">epoch=1, lrate=0.010, error=89.115\n",
      ">epoch=2, lrate=0.010, error=86.217\n",
      ">epoch=3, lrate=0.010, error=84.702\n",
      ">epoch=4, lrate=0.010, error=83.842\n",
      ">epoch=5, lrate=0.010, error=83.336\n",
      ">epoch=6, lrate=0.010, error=83.029\n",
      ">epoch=7, lrate=0.010, error=82.835\n",
      ">epoch=8, lrate=0.010, error=82.709\n",
      ">epoch=9, lrate=0.010, error=82.624\n",
      ">epoch=10, lrate=0.010, error=82.564\n",
      ">epoch=11, lrate=0.010, error=82.520\n",
      ">epoch=12, lrate=0.010, error=82.487\n",
      ">epoch=13, lrate=0.010, error=82.461\n",
      ">epoch=14, lrate=0.010, error=82.441\n",
      ">epoch=15, lrate=0.010, error=82.424\n",
      ">epoch=16, lrate=0.010, error=82.409\n",
      ">epoch=17, lrate=0.010, error=82.397\n",
      ">epoch=18, lrate=0.010, error=82.386\n",
      ">epoch=19, lrate=0.010, error=82.376\n",
      ">epoch=20, lrate=0.010, error=82.367\n",
      ">epoch=21, lrate=0.010, error=82.359\n",
      ">epoch=22, lrate=0.010, error=82.351\n",
      ">epoch=23, lrate=0.010, error=82.344\n",
      ">epoch=24, lrate=0.010, error=82.337\n",
      ">epoch=25, lrate=0.010, error=82.330\n",
      ">epoch=26, lrate=0.010, error=82.324\n",
      ">epoch=27, lrate=0.010, error=82.317\n",
      ">epoch=28, lrate=0.010, error=82.311\n",
      ">epoch=29, lrate=0.010, error=82.305\n",
      ">epoch=30, lrate=0.010, error=82.299\n",
      ">epoch=31, lrate=0.010, error=82.294\n",
      ">epoch=32, lrate=0.010, error=82.288\n",
      ">epoch=33, lrate=0.010, error=82.282\n",
      ">epoch=34, lrate=0.010, error=82.277\n",
      ">epoch=35, lrate=0.010, error=82.271\n",
      ">epoch=36, lrate=0.010, error=82.266\n",
      ">epoch=37, lrate=0.010, error=82.261\n",
      ">epoch=38, lrate=0.010, error=82.255\n",
      ">epoch=39, lrate=0.010, error=82.250\n",
      ">epoch=40, lrate=0.010, error=82.245\n",
      ">epoch=41, lrate=0.010, error=82.240\n",
      ">epoch=42, lrate=0.010, error=82.234\n",
      ">epoch=43, lrate=0.010, error=82.229\n",
      ">epoch=44, lrate=0.010, error=82.224\n",
      ">epoch=45, lrate=0.010, error=82.219\n",
      ">epoch=46, lrate=0.010, error=82.214\n",
      ">epoch=47, lrate=0.010, error=82.209\n",
      ">epoch=48, lrate=0.010, error=82.204\n",
      ">epoch=49, lrate=0.010, error=82.199\n",
      ">epoch=0, lrate=0.010, error=105.251\n",
      ">epoch=1, lrate=0.010, error=89.828\n",
      ">epoch=2, lrate=0.010, error=86.809\n",
      ">epoch=3, lrate=0.010, error=85.247\n",
      ">epoch=4, lrate=0.010, error=84.367\n",
      ">epoch=5, lrate=0.010, error=83.853\n",
      ">epoch=6, lrate=0.010, error=83.542\n",
      ">epoch=7, lrate=0.010, error=83.348\n",
      ">epoch=8, lrate=0.010, error=83.222\n",
      ">epoch=9, lrate=0.010, error=83.137\n",
      ">epoch=10, lrate=0.010, error=83.077\n",
      ">epoch=11, lrate=0.010, error=83.033\n",
      ">epoch=12, lrate=0.010, error=83.000\n",
      ">epoch=13, lrate=0.010, error=82.973\n",
      ">epoch=14, lrate=0.010, error=82.952\n",
      ">epoch=15, lrate=0.010, error=82.934\n",
      ">epoch=16, lrate=0.010, error=82.919\n",
      ">epoch=17, lrate=0.010, error=82.906\n",
      ">epoch=18, lrate=0.010, error=82.894\n",
      ">epoch=19, lrate=0.010, error=82.883\n",
      ">epoch=20, lrate=0.010, error=82.873\n",
      ">epoch=21, lrate=0.010, error=82.863\n",
      ">epoch=22, lrate=0.010, error=82.854\n",
      ">epoch=23, lrate=0.010, error=82.846\n",
      ">epoch=24, lrate=0.010, error=82.838\n",
      ">epoch=25, lrate=0.010, error=82.830\n",
      ">epoch=26, lrate=0.010, error=82.823\n",
      ">epoch=27, lrate=0.010, error=82.816\n",
      ">epoch=28, lrate=0.010, error=82.808\n",
      ">epoch=29, lrate=0.010, error=82.802\n",
      ">epoch=30, lrate=0.010, error=82.795\n",
      ">epoch=31, lrate=0.010, error=82.788\n",
      ">epoch=32, lrate=0.010, error=82.782\n",
      ">epoch=33, lrate=0.010, error=82.775\n",
      ">epoch=34, lrate=0.010, error=82.769\n",
      ">epoch=35, lrate=0.010, error=82.763\n",
      ">epoch=36, lrate=0.010, error=82.756\n",
      ">epoch=37, lrate=0.010, error=82.750\n",
      ">epoch=38, lrate=0.010, error=82.744\n",
      ">epoch=39, lrate=0.010, error=82.738\n",
      ">epoch=40, lrate=0.010, error=82.732\n",
      ">epoch=41, lrate=0.010, error=82.727\n",
      ">epoch=42, lrate=0.010, error=82.721\n",
      ">epoch=43, lrate=0.010, error=82.715\n",
      ">epoch=44, lrate=0.010, error=82.709\n",
      ">epoch=45, lrate=0.010, error=82.703\n",
      ">epoch=46, lrate=0.010, error=82.698\n",
      ">epoch=47, lrate=0.010, error=82.692\n",
      ">epoch=48, lrate=0.010, error=82.687\n",
      ">epoch=49, lrate=0.010, error=82.681\n",
      "Scores: [0.14735540446555573, 0.15903697774568384, 0.15233493909749518, 0.15726939062869139, 0.12712054726997885]\n",
      "Mean RMSE: 0.149\n"
     ]
    }
   ],
   "source": [
    "# evaluate algorithm\n",
    "n_folds = 5\n",
    "l_rate = 0.01\n",
    "n_epoch = 50\n",
    "scores = evaluate_algorithm(dataset, linear_regression_sgd, n_folds, l_rate, n_epoch)\n",
    "print('Scores: %s' % scores)\n",
    "print('Mean RMSE: %.3f' % (sum(scores)/float(len(scores))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Refference : https://machinelearningmastery.com/implement-linear-regression-stochastic-gradient-descent-scratch-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
